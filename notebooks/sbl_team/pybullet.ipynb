{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# Stable Baselines3 - PyBullet: Normalizing Features and Reward\n",
    "\n",
    "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
    "\n",
    "\n",
    "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
    "\n",
    "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
    "\n",
    "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
    "\n",
    "Pybullet source code: https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/\n",
    "\n",
    "## Install Dependencies and Stable Baselines Using Pip\n",
    "\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWskDE2c9WoN"
   },
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3[extra] pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path_to_curr_file=os.path.realpath(os.getcwd())\n",
    "proj_root=os.path.dirname(os.path.dirname(path_to_curr_file))\n",
    "if proj_root not in sys.path:\n",
    "    sys.path.insert(0,proj_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtY8FhliLsGm"
   },
   "source": [
    "## Import policy, RL agent, Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIedd7Pz9sOs"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import pybullet_envs\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7c8VHsiXC7dL"
   },
   "source": [
    "## Create and wrap the environment with `VecNormalize`\n",
    "\n",
    "Normalizing input features may be essential to successful training of an RL agent (by default, images are scaled but not other types of input), for instance when training on [PyBullet](https://github.com/bulletphysics/bullet3/) environments. For that, a wrapper exists and will compute a running average and standard deviation of input features (it can do the same for rewards).\n",
    "\n",
    "More information about `VecNormalize`:\n",
    "- [Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#stable_baselines3.common.vec_env.VecNormalize)\n",
    "- [Discussion](https://github.com/hill-a/stable-baselines/issues/698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmxIq5UeC3Nj"
   },
   "outputs": [],
   "source": [
    "env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\n",
    "\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxUMGsl5mabF"
   },
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQmsSZUHKNRG"
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZYBIVoLmcR4"
   },
   "source": [
    "###Â Save the agent and the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpMDXP0vmezv"
   },
   "outputs": [],
   "source": [
    "# Don't forget to save the VecNormalize statistics when saving the agent\n",
    "log_dir = \"/tmp/\"\n",
    "model.save(log_dir + \"ppo_halfcheetah\")\n",
    "stats_path = os.path.join(log_dir, \"vec_normalize.pkl\")\n",
    "env.save(stats_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eezphIrRmr-Y"
   },
   "source": [
    "### Test model: load the saved agent and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQT1k7lWmmTL"
   },
   "outputs": [],
   "source": [
    "# Load the agent\n",
    "model = PPO.load(log_dir + \"ppo_halfcheetah\")\n",
    "\n",
    "# Load the saved statistics\n",
    "env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\n",
    "env = VecNormalize.load(stats_path, env)\n",
    "#  do not update them at test time\n",
    "env.training = False\n",
    "# reward normalization is not needed at test time\n",
    "env.norm_reward = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "voCxMxfYm2yc"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zC2IMJUm3Kw"
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tHTCBIynFhd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pybullet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
